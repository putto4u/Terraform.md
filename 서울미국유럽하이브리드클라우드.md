## π› οΈ ν•μ΄λΈλ¦¬λ“ ν΄λ¬μ¤ν„° κµ¬μ¶• λ‹¨κ³„λ³„ μ‘μ—… ν†µν•© μ”μ•½



μ΄ μ‹λ‚λ¦¬μ¤μ—μ„λ” **VMware vSphere** ν™κ²½μ„ κ°€μ •ν•μ—¬ λ³Έμ‚¬ μ„λ²„λ¥Ό **Terraform**μΌλ΅ μ •μν•κ³ , **Ansible**μ„ μ‚¬μ©ν•μ—¬ K8s μ„¤μΉ λ° Join λ…λ Ήμ„ μλ™ν™”ν•λ” κ°€μ¥ **μ „λ¬Έμ μΈ λ°©μ‹**μ„ μ±„νƒν•©λ‹λ‹¤.

### π“ μµμΆ… μ‘μ—… νλ¦„ λ° λ„κµ¬

| \# | μ‘μ—… μμ—­ | λ€μƒ μ¥λΉ„/μ„λΉ„μ¤ | μ£Όμ” μ‘μ—… λ‚΄μ© | λ‹΄λ‹Ή λ„κµ¬ |
| :---: | :---: | :---: | :---: | :---: |
| **1λ‹¨κ³„** | **μΈν”„λΌ μ •μ (IaC)** | **VMware vSphere, AWS (3 λ¦¬μ „)** | **λ³Έμ‚¬ Master VM**, **AWS Worker EC2 (3λ€)** λ° **VPC/λ„¤νΈμ›ν¬** μμ› μ •μ. | **Terraform (vSphere, AWS)** |
| **2λ‹¨κ³„** | **λΌμ°ν„° μ„¤μ •** | **λ³Έμ‚¬ λΌμ°ν„°** | \*\*AWS VPC λ€μ—­($10.10.0.0/16$ λ“±)\*\*μΌλ΅ ν–¥ν•λ” **μ •μ  κ²½λ΅(Static Route)** λ° **ACL** μ„¤μ •. (VPN/Direct Connect ν™κ²½ κ°€μ •) | **Cisco IOS/CLI** |
| **3λ‹¨κ³„** | **μ΄μμ²΄μ  κµ¬μ„± & μ„¤μΉ** | **λ¨λ“  μ„λ²„ (4λ€)** | **Docker/Containerd, Kubeadm** μ„¤μΉ, **Swap λΉ„ν™μ„±ν™”.** | **Terraform** (`user_data`) + **Ansible** |
| **4λ‹¨κ³„** | **ν΄λ¬μ¤ν„° κµ¬μ„± & Join** | **λ¨λ“  μ„λ²„ (4λ€)** | **Master μ΄κΈ°ν™”, CNI μ„¤μΉ, Worker Join** λ…λ Ή μ‹¤ν–‰μ„ **μλ™ν™”**. | **Ansible** |
| **5λ‹¨κ³„** | **λ΅λ“ λ°Έλ°μ‹± μΈν”„λΌ** | **AWS (3 λ¦¬μ „) & Route 53** | λ¦¬μ „λ³„ **NLB/ALB** μƒμ„±, **AWS Route 53 GSLB** μ„¤μ •. | **Terraform (AWS)** |

-----

## 1\. π“ 1λ‹¨κ³„: μΈν”„λΌ μ •μ (Terraform μ½”λ“)

### A. λ³Έμ‚¬ μ„λ²„ μ •μ (VMware vSphere κ°€μ •)

VMware vCenterμ— μ ‘μ†ν•μ—¬ K8s Master μ—­ν• μ„ μν–‰ν•  VMμ„ μ •μν•©λ‹λ‹¤.

```terraform
# main.tf (VMware Provider μ„¤μ • λ¶€λ¶„)
terraform {
  required_providers {
    aws = { source = "hashicorp/aws" }
    vsphere = { source = "hashicorp/vsphere" }
  }
}

provider "vsphere" {
  user           = var.vsphere_user
  password       = var.vsphere_password
  vsphere_server = var.vsphere_server
  allow_unverified_ssl = true
}

# ------------------------------------------
# 1. λ³Έμ‚¬ K8s Master VM μ •μ (VMware vSphere)
# ------------------------------------------
data "vsphere_datacenter" "dc" { name = "Datacenter-A" }
data "vsphere_resource_pool" "pool" {
  name          = "Cluster-A/Resources"
  datacenter_id = data.vsphere_datacenter.dc.id
}
data "vsphere_datastore" "datastore" {
  name          = "Datastore-Prod"
  datacenter_id = data.vsphere_datacenter.dc.id
}
data "vsphere_network" "network" {
  name          = "Prod-LAN"
  datacenter_id = data.vsphere_datacenter.dc.id
}

resource "vsphere_virtual_machine" "k8s_master" {
  name             = "K8s-Master-OnPrem"
  resource_pool_id = data.vsphere_resource_pool.pool.id
  datastore_id     = data.vsphere_datastore.datastore.id
  num_cpus         = 4
  memory           = 8192
  guest_id         = "ubuntu64Guest"
  scsi_type        = "lsilogic"

  # NIC μ„¤μ • (λ³Έμ‚¬ λ‚΄λ¶€λ§: 192.168.1.100/24 κ³ μ • IP κ°€μ •)
  network_interface {
    network_id = data.vsphere_network.network.id
    adapter_type = "e1000"
  }
  disk {
    label = "disk0"
    size  = 100
    eagerly_scrub = false
    thin_provisioned = true
  }
  
  # 3λ‹¨κ³„: OS λ‚΄λ¶€ μ„¤μ • (Cloud-init λ€μ²΄)
  clone {
    template_uuid = "YOUR_UBUNTU_TEMPLATE_UUID" # Ubuntu VM ν…ν”λ¦Ώ UUID
  }

  # VMμ— SSH ν‚¤ λ° μ΄κΈ° μ„¤μ • μ¤ν¬λ¦½νΈ μ „λ‹¬ (Ansibleμ„ μ„ν•΄ SSH μ„¤μ • ν•„μ)
  # (μ΄ν• μƒλµ - vsphere providerμ user_dataλ” λ³µμ΅ν•μ—¬ Ansibleλ΅ λ€μ²΄ κ¶μ¥)
}
```

### B. AWS Worker μΈν”„λΌ μ •μ (`main.tf` ν†µν•©)

μ΄μ „ λ‹µλ³€μ AWS EC2, VPC, NLB, Route 53 μ„¤μ •μ„ λ¨λ‘ `main.tf`μ— ν†µν•©ν•©λ‹λ‹¤. (μ½”λ“κ°€ κΈΈμ–΄μ§€λ―€λ΅ λ¨λ“ νμΌ λ‚΄λ¶€λ” μ΄μ „ λ‹µλ³€κ³Ό λ™μΌν•λ‹¤κ³  κ°€μ •ν•©λ‹λ‹¤.)

```terraform
# main.tf (AWS λ¦¬μ†μ¤ μ •μ λ¶€λ¶„ - ν†µν•©)

# ... (Provider μ„¤μ • μƒλµ) ...

# ------------------------------------------
# 2. AWS Worker μΈν”„λΌ μ •μ (VPC, EC2, SG)
# ------------------------------------------
module "seoul_infra" {
  source    = "./modules/regional_setup"
  providers = { aws = aws.seoul }
  # ... (λ³€μ μ„¤μ • μƒλµ) ...
}
# (module "virginia_infra" λ° "frankfurt_infra" νΈμ¶ μƒλµ)

# ------------------------------------------
# 5. λ΅λ“ λ°Έλ°μ‹± μΈν”„λΌ μ •μ
# ------------------------------------------

# λ¦¬μ „λ³„ NLB μ •μ
resource "aws_lb" "seoul_nlb" {
  provider = aws.seoul
  # ... (NLB μ„¤μ • λ° Target Group, Attachment μ •μ μƒλµ) ...
  # Target ID: module.seoul_infra.instance_id μ‚¬μ©
}
# (λ―Έμ£Ό λ° μ λ½ NLB μ •μ μƒλµ)

# Route 53 GSLB μ •μ
resource "aws_route53_record" "global_k8s_access" {
  # ... (GSLB μ„¤μ • μƒλµ) ...
  # Alias Name: aws_lb.seoul_nlb.dns_name μ‚¬μ©
}
```

-----

## 2\. π“ 2λ‹¨κ³„: λ³Έμ‚¬ λΌμ°ν„° μ„¤μ • μ½”λ“

### β™οΈ μ„μΈ λ³Έμ‚¬ λΌμ°ν„° μ„¤μ • (Cisco IOS κΈ°λ°)

λ³Έμ‚¬ λΌμ°ν„°κ°€ AWSμ μ„Έ VPC λ€μ—­μΌλ΅ νΈλν”½μ„ μ •ν™•ν λΌμ°ν…ν•λ„λ΅ μ„¤μ •ν•©λ‹λ‹¤. (AWS VPCμ™€ λ³Έμ‚¬ κ°„ VPN λλ” Direct Connect μ—°κ²°μ΄ `GigabitEthernet0/0` μΈν„°νμ΄μ¤λ¥Ό ν†µν•΄ μ—°κ²°λμ—λ‹¤κ³  κ°€μ •)

```bash
! Router_SEOUL (μ„μΈ λ³Έμ‚¬ λΌμ°ν„°) μ„¤μ •

configure terminal
hostname Router_SEOUL

! AWS VPC λ€μ—­μΌλ΅μ μ •μ  κ²½λ΅ μ„¤μ •
! λ‹¤μ ν™‰(Next-Hop)μ€ VPN ν„°λ„μ μƒλ€νΈ κ²μ΄νΈμ›¨μ΄ IP μ£Όμ†μ…λ‹λ‹¤. (κ°€μƒ IP: 100.1.1.1)
ip route 10.10.0.0 255.255.0.0 100.1.1.1
ip route 10.20.0.0 255.255.0.0 100.1.1.1
ip route 10.30.0.0 255.255.0.0 100.1.1.1

! ACL μ„¤μ •: K8s ν†µμ‹ μ— ν•„μ”ν• ν¬νΈ ν—μ© (6443, 10250 λ“±)
ip access-list extended K8S_HYBRID_ACL
 permit tcp 10.0.0.0 0.255.255.255 host 192.168.1.100 eq 6443  ! AWS -> Master API
 permit tcp 10.0.0.0 0.255.255.255 192.168.1.0 0.0.0.255 eq 10250  ! AWS <-> OnPrem Kubelet
 permit tcp 10.0.0.0 0.255.255.255 192.168.1.0 0.0.0.255 eq 30000 32767 ! NodePort Range
! (CNI ν†µμ‹ , ETCD ν†µμ‹  λ“±μ— ν•„μ”ν• ν¬νΈ μ¶”κ°€ ν•„μ”)

interface GigabitEthernet0/0
 ip access-group K8S_HYBRID_ACL in
 
end
write memory
```

-----

## 3\. π“ 3λ‹¨κ³„ & 4λ‹¨κ³„: μ΄μμ²΄μ  κµ¬μ„± λ° ν΄λ¬μ¤ν„° κµ¬μ„± (Ansible μ½”λ“)

Ansibleμ€ μ΄λ―Έ TerraformμΌλ΅ μƒμ„±λ VM λ° EC2μ— μ ‘μ†ν•μ—¬ K8s μ„¤μΉ λ° ν΄λ¬μ¤ν„° Join λ…λ Ήμ„ μ‹¤ν–‰ν•©λ‹λ‹¤.

### A. π΅ Ansible μΈλ²¤ν† λ¦¬ (`inventory.ini`)

```ini
# inventory.ini

[kube_master]
k8s_master_onprem ansible_host=192.168.1.100 ansible_user=ubuntu

[kube_workers]
k8s_worker_seoul ansible_host=[μ„μΈ EC2 κ³µμΈIP] ansible_user=ubuntu
k8s_worker_us ansible_host=[λ―Έκµ­ EC2 κ³µμΈIP] ansible_user=ubuntu
k8s_worker_eu ansible_host=[μ λ½ EC2 κ³µμΈIP] ansible_user=ubuntu

[k8s_cluster:children]
kube_master
kube_workers
```

### B. π³ Ansible ν”λ μ΄λ¶ (`site.yaml`)

μ΄ ν”λ μ΄λ¶μ€ λ¨λ“  λ…Έλ“μ— K8sλ¥Ό μ„¤μΉν•κ³ , Master μ΄κΈ°ν™” ν›„ Workerλ¥Ό Join μ‹ν‚µλ‹λ‹¤.

```yaml
# site.yaml

---
# ------------------------------------------
# Play 1: λ¨λ“  λ…Έλ“μ— Docker/Kubeadm μ„¤μΉ (3λ‹¨κ³„ μλ™ν™”)
# ------------------------------------------
- name: 3. Install Kubeadm and Docker
  hosts: k8s_cluster
  become: yes
  tasks:
    - name: Disable swap
      shell: |
        swapoff -a
        sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab

    - name: Install required packages
      apt:
        name: "{{ item }}"
        state: present
      loop:
        - apt-transport-https
        - ca-certificates
        - curl
        - gnupg
        - lsb-release

    - name: Install Containerd (Docker runtime)
      shell: |
        curl -fsSL https://download.docker.com/linux/ubuntu/gpg | gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg
        echo "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable" | tee /etc/apt/sources.list.d/docker.list > /dev/null
        apt update
        apt install -y containerd.io

    - name: Install Kubeadm, Kubelet, Kubectl
      apt:
        name: kubelet={{ kube_version }}, kubeadm={{ kube_version }}, kubectl={{ kube_version }}
        state: present
        update_cache: yes
      vars:
        kube_version: "1.28.0-00" # μ›ν•λ” K8s λ²„μ „ μ§€μ •

# ------------------------------------------
# Play 2: Master μ΄κΈ°ν™” λ° CNI μ„¤μΉ (4λ‹¨κ³„)
# ------------------------------------------
- name: 4. Initialize K8s Master and Install CNI
  hosts: kube_master
  become: yes
  tasks:
    - name: Initialize Kubernetes cluster
      shell: |
        kubeadm init --control-plane-endpoint "{{ hostvars['k8s_master_onprem']['ansible_host'] }}:6443" --pod-network-cidr="172.16.0.0/16" --upload-certs
      register: kubeadm_init_result

    - name: Set up kubeconfig for root and user
      shell: |
        mkdir -p $HOME/.kube
        cp /etc/kubernetes/admin.conf $HOME/.kube/config
        chown $(id -u):$(id -g) $HOME/.kube/config
        
    - name: Install Calico CNI
      shell: kubectl apply -f https://raw.githubusercontent.com/projectcalico/calico/v3.25.0/manifests/calico.yaml
      
    - name: Get Join Command
      shell: kubeadm token create --print-join-command
      register: join_command

    - name: Store Join Command for Workers
      set_fact:
        worker_join_command: "{{ join_command.stdout }}"

# ------------------------------------------
# Play 3: Worker λ…Έλ“ ν΄λ¬μ¤ν„° Join (4λ‹¨κ³„)
# ------------------------------------------
- name: 4. Join Workers to the Cluster
  hosts: kube_workers
  become: yes
  tasks:
    - name: Execute join command
      shell: "{{ hostvars['k8s_master_onprem']['worker_join_command'] }}"
      args:
        warn: no
```

**μ‹¤ν–‰ μμ„:**

1.  **Terraform Apply:** `terraform init` -\> `terraform apply` (AWS μΈν”„λΌ, λ³Έμ‚¬ VM μƒμ„±)
2.  **λΌμ°ν„° μ„¤μ •:** μλ™μΌλ΅ Cisco IOSμ— μ„ `Router_SEOUL` μ¤ν¬λ¦½νΈ μ μ©.
3.  **Ansible μ‹¤ν–‰:** `ansible-playbook -i inventory.ini site.yaml` μ‹¤ν–‰ (K8s μ„¤μΉ λ° ν΄λ¬μ¤ν„° ν†µν•©).

μ΄ κ³Όμ •μ„ ν†µν•΄ **μΈν”„λΌλ¶€ν„° ν΄λ¬μ¤ν„° κµ¬μ„±**κΉμ§€ **IaC κΈ°λ°μ μλ™ν™”λ λ¶„μ‚° μ‹μ¤ν… κµ¬μ¶•**μ΄ μ™„λ£λ©λ‹λ‹¤.
